import logging
import os

from langchain_aws import ChatBedrockConverse
from langchain_core.prompts import ChatPromptTemplate

from app.core.config import settings
from app.core.exceptions import AIGenerationException, AIModelNotAvailableException
from app.core.prompts import (
    ANALYZE_ANSWER_PROMPT,
    GENERATE_REACTION_PROMPT,
    GENERATE_TAIL_QUESTION_PROMPT,
    QUESTION_FEEDBACK_SYSTEM_PROMPT,
    QUESTION_GENERATION_SYSTEM_PROMPT,
)
from app.schemas.fixed_question import (
    FixedQuestionDraft,
    FixedQuestionDraftCreate,
    FixedQuestionFeedback,
    FixedQuestionFeedbackCreate,
)
from app.schemas.survey import AnswerAnalysis

logger = logging.getLogger(__name__)


class BedrockService:
    """AWS Bedrock API ë˜í¼ (LangChain ê¸°ë°˜)"""

    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if settings.AWS_BEDROCK_API_KEY:
            os.environ["AWS_BEARER_TOKEN_BEDROCK"] = settings.AWS_BEDROCK_API_KEY

        try:
            self.chat_model = ChatBedrockConverse(
                model=settings.BEDROCK_MODEL_ID,
                temperature=settings.TEMPERATURE,
                max_tokens=settings.MAX_TOKENS,
                region_name=settings.AWS_REGION,
            )
        except Exception as error:
            logger.error(
                f"âŒ Bedrock ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {type(error).__name__}: {error}"
            )
            raise AIModelNotAvailableException(
                f"Bedrock ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error}"
            ) from error

    def invoke(self, prompt: str) -> str:
        """ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ (ì—°ê²° í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            response = self.chat_model.invoke(prompt)
            return response.content
        except Exception as error:
            logger.error(f"âŒ Bedrock API ì—ëŸ¬: {type(error).__name__}: {error}")
            raise AIGenerationException(f"Bedrock API í˜¸ì¶œ ì‹¤íŒ¨: {error}") from error

    async def generate_fixed_questions(
        self, request: FixedQuestionDraftCreate
    ) -> FixedQuestionDraft:
        """ê²Œì„ ì •ë³´ ê¸°ë°˜ ê³ ì • ì§ˆë¬¸ ìƒì„±."""
        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_GENERATION_SYSTEM_PROMPT)
            structured_llm = self.chat_model.with_structured_output(FixedQuestionDraft)
            chain = prompt | structured_llm

            theme_info = self._format_theme_info(
                request.theme_priorities, request.theme_details
            )

            return await chain.ainvoke(
                {
                    "game_name": request.game_name,
                    "game_genre": request.game_genre,
                    "game_context": request.game_context,
                    "theme_info": theme_info,
                }
            )

        except AIGenerationException:
            raise
        except Exception as error:
            logger.error(f"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {error}")
            raise AIGenerationException(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error

    async def generate_feedback_questions(
        self, request: FixedQuestionFeedbackCreate
    ) -> FixedQuestionFeedback:
        """í”¼ë“œë°±ì„ ë°˜ì˜í•œ ëŒ€ì•ˆ ì§ˆë¬¸ 3ê°œ ìƒì„±."""
        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_FEEDBACK_SYSTEM_PROMPT)
            structured_llm = self.chat_model.with_structured_output(
                FixedQuestionFeedback
            )
            chain = prompt | structured_llm

            theme_info = self._format_theme_info(
                request.theme_priorities, request.theme_details
            )

            return await chain.ainvoke(
                {
                    "game_name": request.game_name,
                    "game_genre": request.game_genre,
                    "game_context": request.game_context,
                    "theme_info": theme_info,
                    "original_question": request.original_question,
                }
            )

        except AIGenerationException:
            raise
        except Exception as error:
            logger.error(f"âŒ ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {error}")
            raise AIGenerationException(
                f"ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}"
            ) from error

    def analyze_answer(
        self,
        current_question: str,
        user_answer: str,
        tail_question_count: int,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> dict:
        """ë‹µë³€ ë¶„ì„ í›„ TAIL_QUESTION ë˜ëŠ” PASS_TO_NEXT ê²°ì •."""
        try:
            prompt = ChatPromptTemplate.from_template(ANALYZE_ANSWER_PROMPT)
            structured_llm = self.chat_model.with_structured_output(AnswerAnalysis)
            chain = prompt | structured_llm

            result: AnswerAnalysis = chain.invoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "tail_question_count": tail_question_count,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return {"action": result.action.value, "analysis": result.analysis}

        except Exception as error:
            logger.error(f"âŒ ë‹µë³€ ë¶„ì„ ì‹¤íŒ¨: {error}")
            raise AIGenerationException(f"ë‹µë³€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error

    def generate_tail_question(
        self,
        current_question: str,
        user_answer: str,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> str:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_TAIL_QUESTION_PROMPT)
            chain = prompt | self.chat_model

            response = chain.invoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return response.content

        except Exception as error:
            logger.error(f"âŒ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {error}")
            raise AIGenerationException(
                f"ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}"
            ) from error

    def _format_history(self, history: list[dict] | None) -> str:
        """ëŒ€í™” ê¸°ë¡ì„ LLMì´ ì½ê¸° ì‰¬ìš´ í¬ë§·ìœ¼ë¡œ ë³€í™˜."""
        if not history:
            return "ì—†ìŒ"

        formatted = []
        for i, entry in enumerate(history, 1):
            formatted.append(f"{i}. Q: {entry.get('question', 'N/A')}")
            formatted.append(f"   A: {entry.get('answer', 'N/A')}")

        return "\n".join(formatted)

    def _format_theme_info(
        self, theme_priorities: list[str], theme_details: dict[str, list[str]] | None
    ) -> str:
        """ëŒ€ì£¼ì œ ìš°ì„ ìˆœìœ„ì™€ ì„¸ë¶€ í‚¤ì›Œë“œë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜."""
        formatted = []
        details_dict = theme_details or {}
        for i, theme in enumerate(theme_priorities, 1):
            details = ", ".join(details_dict.get(theme, []))
            if details:
                formatted.append(f"{i}ìˆœìœ„. {theme}: [{details}]")
            else:
                formatted.append(f"{i}ìˆœìœ„. {theme}")
        return "\n".join(formatted)

    # ============================================================
    # Async Methods for SSE Streaming
    # ============================================================

    async def analyze_answer_async(
        self,
        current_question: str,
        user_answer: str,
        tail_question_count: int,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> dict:
        """ë‹µë³€ ë¶„ì„ í›„ TAIL_QUESTION ë˜ëŠ” PASS_TO_NEXT ê²°ì • (ë¹„ë™ê¸°)."""
        try:
            prompt = ChatPromptTemplate.from_template(ANALYZE_ANSWER_PROMPT)
            structured_llm = self.chat_model.with_structured_output(AnswerAnalysis)
            chain = prompt | structured_llm

            result: AnswerAnalysis = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "tail_question_count": tail_question_count,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return {"action": result.action.value, "analysis": result.analysis}

        except Exception as error:
            logger.error(f"âŒ ë‹µë³€ ë¶„ì„ ì‹¤íŒ¨ (async): {error}")
            raise AIGenerationException(f"ë‹µë³€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error

    async def generate_tail_question_async(
        self,
        current_question: str,
        user_answer: str,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> str:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± (ë¹„ë™ê¸°)."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_TAIL_QUESTION_PROMPT)
            chain = prompt | self.chat_model

            response = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return response.content

        except Exception as error:
            logger.error(f"âŒ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (async): {error}")
            raise AIGenerationException(
                f"ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}"
            ) from error

    async def generate_reaction_async(self, user_answer: str) -> str:
        """ì‚¬ìš©ì ë‹µë³€ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¦¬ì•¡ì…˜ ìƒì„± (DB ì €ì¥ X, UI í‘œì‹œìš©)."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_REACTION_PROMPT)
            chain = prompt | self.chat_model

            response = await chain.ainvoke({"user_answer": user_answer})
            return response.content.strip()

        except Exception as error:
            logger.error(f"âŒ ë¦¬ì•¡ì…˜ ìƒì„± ì‹¤íŒ¨: {error}")
            # ë¦¬ì•¡ì…˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ì—ëŸ¬ throw ì•ˆ í•¨)
            return "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ™"

    async def stream_tail_question(
        self,
        current_question: str,
        user_answer: str,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ):
        """ê¼¬ë¦¬ ì§ˆë¬¸ í† í° ìŠ¤íŠ¸ë¦¬ë° (Gemini/Claude ìŠ¤íƒ€ì¼)."""
        prompt = ChatPromptTemplate.from_template(GENERATE_TAIL_QUESTION_PROMPT)
        chain = prompt | self.chat_model

        async for chunk in chain.astream(
            {
                "current_question": current_question,
                "user_answer": user_answer,
                "game_name": game_info.get("game_name") if game_info else "Unknown",
                "game_genre": game_info.get("game_genre") if game_info else "Unknown",
                "game_context": game_info.get("game_context")
                if game_info
                else "No context",
                "conversation_history": self._format_history(conversation_history),
            }
        ):
            content = chunk.content
            if content:
                # Bedrockì€ contentë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
                if isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and "text" in item:
                            yield item["text"]
                        elif isinstance(item, str):
                            yield item
                else:
                    yield content
