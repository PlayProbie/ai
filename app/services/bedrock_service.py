import logging
import os

from langchain_aws import ChatBedrockConverse
from langchain_core.prompts import ChatPromptTemplate

from app.core.config import settings
from app.core.exceptions import AIGenerationException, AIModelNotAvailableException
from app.core.prompts import (
    ANALYZE_ANSWER_PROMPT,
    GENERATE_REACTION_PROMPT,
    GENERATE_TAIL_QUESTION_PROMPT,
    QUESTION_FEEDBACK_SYSTEM_PROMPT,
    QUESTION_GENERATION_SYSTEM_PROMPT,
    QUESTION_RAG_PROMPT,
)
from app.core.retry_policy import bedrock_retry
from app.schemas.fixed_question import (
    FixedQuestionDraft,
    FixedQuestionDraftCreate,
    FixedQuestionFeedback,
    FixedQuestionFeedbackCreate,
)
from app.schemas.survey import AnswerAnalysis # ì‚­ì œ ê°€ëŠ¥í•´ì§€ë©´ ì‚­ì œ
from app.schemas.survey import ValidityType, ValidityResult
from app.schemas.survey import QualityResult, QualityType

logger = logging.getLogger(__name__)


class BedrockService:
    """AWS Bedrock API ë˜í¼ (LangChain ê¸°ë°˜)"""

    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if settings.AWS_BEDROCK_API_KEY:
            os.environ["AWS_BEARER_TOKEN_BEDROCK"] = settings.AWS_BEDROCK_API_KEY

        try:
            self.chat_model = ChatBedrockConverse(
                model=settings.BEDROCK_MODEL_ID,
                temperature=settings.TEMPERATURE,
                max_tokens=settings.MAX_TOKENS,
                region_name=settings.BEDROCK_REGION,
            )
        except Exception as error:
            logger.error(
                f"âŒ Bedrock ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {type(error).__name__}: {error}"
            )
            raise AIModelNotAvailableException(
                f"Bedrock ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error}"
            ) from error

    @bedrock_retry
    def invoke(self, prompt: str) -> str:
        """ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ (ì—°ê²° í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            response = self.chat_model.invoke(prompt)
            return response.content
        except Exception as error:
            logger.error(f"âŒ Bedrock API ì—ëŸ¬: {type(error).__name__}: {error}")
            raise AIGenerationException(f"Bedrock API í˜¸ì¶œ ì‹¤íŒ¨: {error}") from error

    @bedrock_retry
    async def generate_fixed_questions(
        self, request: FixedQuestionDraftCreate
    ) -> FixedQuestionDraft:
        """ê²Œì„ ì •ë³´ ê¸°ë°˜ ê³ ì • ì§ˆë¬¸ ìƒì„±."""
        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_GENERATION_SYSTEM_PROMPT)
            structured_llm = self.chat_model.with_structured_output(FixedQuestionDraft)
            chain = prompt | structured_llm

            theme_info = self._format_theme_info(
                request.theme_priorities, request.theme_details
            )

            return await chain.ainvoke(
                {
                    "game_name": request.game_name,
                    "game_genre": request.game_genre,
                    "game_context": request.game_context,
                    "theme_info": theme_info,
                }
            )

        except AIGenerationException:
            raise
        except Exception as error:
            logger.error(f"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {error}")
            raise AIGenerationException(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error

    @bedrock_retry
    async def generate_feedback_questions(
        self, request: FixedQuestionFeedbackCreate
    ) -> FixedQuestionFeedback:
        """í”¼ë“œë°±ì„ ë°˜ì˜í•œ ëŒ€ì•ˆ ì§ˆë¬¸ 3ê°œ ìƒì„±."""
        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_FEEDBACK_SYSTEM_PROMPT)
            structured_llm = self.chat_model.with_structured_output(
                FixedQuestionFeedback
            )
            chain = prompt | structured_llm

            theme_info = self._format_theme_info(
                request.theme_priorities, request.theme_details
            )

            return await chain.ainvoke(
                {
                    "game_name": request.game_name,
                    "game_genre": request.game_genre,
                    "game_context": request.game_context,
                    "theme_info": theme_info,
                    "original_question": request.original_question,
                }
            )

        except AIGenerationException:
            raise
        except Exception as error:
            logger.error(f"âŒ ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {error}")
            raise AIGenerationException(
                f"ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}"
            ) from error

    @bedrock_retry
    async def evaluate_validity_async(
        self,
        answer: str,
        current_question: str,
    ) -> ValidityResult:
        """LLM ê¸°ë°˜ ì‘ë‹µ ìœ íš¨ì„± í‰ê°€ (ë¹„ë™ê¸°)."""
        from app.core.prompts import VALIDITY_EVALUATION_PROMPT

        try:
            prompt = ChatPromptTemplate.from_template(VALIDITY_EVALUATION_PROMPT)
            structured_llm = self.chat_model.with_structured_output(ValidityResult)
            chain = prompt | structured_llm

            result: ValidityResult = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": answer,
                }
            )
            return result

        except Exception as error:
            logger.error(f"âŒ ìœ íš¨ì„± í‰ê°€ ì‹¤íŒ¨: {error}")
            # ì‹¤íŒ¨ ì‹œ VALIDë¡œ í´ë°± (ê´€ëŒ€í•˜ê²Œ)
            return ValidityResult(
                validity=ValidityType.VALID,
                confidence=0.5,
                reason=f"LLM í‰ê°€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: {error}",
                source="fallback",
            )

    @bedrock_retry
    async def generate_rag_questions_async(
        self,
        reference_questions: list[str],
        game_info: dict,
        count: int = 5,
    ) -> list[str]:
        """RAG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (ì°¸ê³  ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ë°˜ì˜)."""
        from typing import List
        from pydantic import BaseModel, Field

        class RagResponse(BaseModel):
            questions: List[str] = Field(description="ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡")

        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_RAG_PROMPT)
            structured_llm = self.chat_model.with_structured_output(RagResponse)
            chain = prompt | structured_llm

            # ê²Œì„ ìš”ì†Œ í¬ë§·íŒ…
            elements = game_info.get("extracted_elements", {})
            elements_str = ", ".join([f"{k}: {v}" for k, v in elements.items()]) if elements else "ì—†ìŒ"

            # ì°¸ê³  ì§ˆë¬¸ í¬ë§·íŒ…
            refs_str = "\n".join([f"- {q}" for q in reference_questions])

            result: RagResponse = await chain.ainvoke(
                {
                    "game_name": game_info.get("game_name", ""),
                    "game_description": game_info.get("game_description", ""),
                    "extracted_elements": elements_str,
                    "reference_questions": refs_str,
                    "count": count,
                }
            )

            return result.questions

        except Exception as error:
            logger.error(f"âŒ RAG ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {error}")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í˜¸ì¶œ ì¸¡ì—ì„œ Fallback ì²˜ë¦¬)
            return []

    @bedrock_retry
    async def evaluate_quality_async(
        self,
        answer: str,
        current_question: str,
        game_context: str,
    ) -> QualityResult:
        """LLM ê¸°ë°˜ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (Thickness Ã— Richness)."""
        from app.core.prompts import QUALITY_EVALUATION_PROMPT

        try:
            prompt = ChatPromptTemplate.from_template(QUALITY_EVALUATION_PROMPT)
            structured_llm = self.chat_model.with_structured_output(QualityResult)
            chain = prompt | structured_llm

            result: QualityResult = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": answer,
                    "game_context": game_context,
                }
            )
            return result

        except Exception as error:
            logger.error(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {error}")
            # ì‹¤íŒ¨ ì‹œ EMPTYë¡œ í´ë°± (ë³´ìˆ˜ì ìœ¼ë¡œ)
            return QualityResult(
                thickness="LOW",
                thickness_evidence=[],
                richness="LOW",
                richness_evidence=[],
                quality=QualityType.EMPTY,
            )

    def _format_history(self, history: list[dict] | None) -> str:
        """ëŒ€í™” ê¸°ë¡ì„ LLMì´ ì½ê¸° ì‰¬ìš´ í¬ë§·ìœ¼ë¡œ ë³€í™˜."""
        if not history:
            return "ì—†ìŒ"

        formatted = []
        for i, entry in enumerate(history, 1):
            formatted.append(f"{i}. Q: {entry.get('question', 'N/A')}")
            formatted.append(f"   A: {entry.get('answer', 'N/A')}")

        return "\n".join(formatted)

    def _format_theme_info(
        self, theme_priorities: list[str], theme_details: dict[str, list[str]] | None
    ) -> str:
        """ëŒ€ì£¼ì œ ìš°ì„ ìˆœìœ„ì™€ ì„¸ë¶€ í‚¤ì›Œë“œë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜."""
        formatted = []
        details_dict = theme_details or {}
        for i, theme in enumerate(theme_priorities, 1):
            details = ", ".join(details_dict.get(theme, []))
            if details:
                formatted.append(f"{i}ìˆœìœ„. {theme}: [{details}]")
            else:
                formatted.append(f"{i}ìˆœìœ„. {theme}")
        return "\n".join(formatted)

    # ============================================================
    # Async Methods for SSE Streaming
    # ============================================================

    @bedrock_retry
    async def analyze_answer_async(
        self,
        current_question: str,
        user_answer: str,
        tail_question_count: int,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> dict:
        """ë‹µë³€ ë¶„ì„ í›„ TAIL_QUESTION ë˜ëŠ” PASS_TO_NEXT ê²°ì • (ë¹„ë™ê¸°)."""
        try:
            prompt = ChatPromptTemplate.from_template(ANALYZE_ANSWER_PROMPT)
            structured_llm = self.chat_model.with_structured_output(AnswerAnalysis)
            chain = prompt | structured_llm

            result: AnswerAnalysis = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "tail_question_count": tail_question_count,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return {"action": result.action.value, "analysis": result.analysis}

        except Exception as error:
            logger.error(f"âŒ ë‹µë³€ ë¶„ì„ ì‹¤íŒ¨ (async): {error}")
            raise AIGenerationException(f"ë‹µë³€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error

    @bedrock_retry
    async def generate_tail_question_async(
        self,
        current_question: str,
        user_answer: str,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ) -> str:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± (ë¹„ë™ê¸°)."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_TAIL_QUESTION_PROMPT)
            chain = prompt | self.chat_model

            response = await chain.ainvoke(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            )

            return response.content

        except Exception as error:
            logger.error(f"âŒ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (async): {error}")
            raise AIGenerationException(
                f"ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}"
            ) from error

    async def generate_reaction_async(self, user_answer: str, current_question: str = "") -> str:
        """ì‚¬ìš©ì ë‹µë³€ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¦¬ì•¡ì…˜ ìƒì„± (DB ì €ì¥ X, UI í‘œì‹œìš©)."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_REACTION_PROMPT)
            chain = prompt | self.chat_model

            response = await chain.ainvoke({
                "user_answer": user_answer,
                "current_question": current_question,
            })
            return response.content.strip()

        except Exception as error:
            logger.error(f"âŒ ë¦¬ì•¡ì…˜ ìƒì„± ì‹¤íŒ¨: {error}")
            # ë¦¬ì•¡ì…˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ì—ëŸ¬ throw ì•ˆ í•¨)
            return "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ™"

    async def stream_tail_question(
        self,
        current_question: str,
        user_answer: str,
        game_info: dict | None = None,
        conversation_history: list[dict] | None = None,
    ):
        """ê¼¬ë¦¬ ì§ˆë¬¸ í† í° ìŠ¤íŠ¸ë¦¬ë° (Gemini/Claude ìŠ¤íƒ€ì¼)."""
        try:
            prompt = ChatPromptTemplate.from_template(GENERATE_TAIL_QUESTION_PROMPT)
            chain = prompt | self.chat_model

            async for chunk in chain.astream(
                {
                    "current_question": current_question,
                    "user_answer": user_answer,
                    "game_name": game_info.get("game_name") if game_info else "Unknown",
                    "game_genre": game_info.get("game_genre")
                    if game_info
                    else "Unknown",
                    "game_context": game_info.get("game_context")
                    if game_info
                    else "No context",
                    "conversation_history": self._format_history(conversation_history),
                }
            ):
                content = chunk.content
                if content:
                    # Bedrockì€ contentë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict) and "text" in item:
                                yield item["text"]
                            elif isinstance(item, str):
                                yield item
                    else:
                        yield content
        except Exception as error:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {error}")
            raise AIGenerationException(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}") from error
