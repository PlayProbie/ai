"""Analytics ì„œë¹„ìŠ¤ - BERTopic ê¸°ë°˜ í† í”½ ëª¨ë¸ë§ + LLM ê°ì • ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
- UMAP ì°¨ì› ì¶•ì†Œ (ê³ ì°¨ì› ì„ë² ë”© â†’ ì €ì°¨ì›)
- HDBSCAN ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ìë™ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ê²°ì • + ë…¸ì´ì¦ˆ ë¶„ë¦¬)
- c-TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
- MMR ê¸°ë°˜ ëŒ€í‘œ ë‹µë³€ ì„ ì • (ìœ ì‚¬ë„ + ë‹¤ì–‘ì„± ê· í˜•)
- ì´ìƒì¹˜(Outlier) ë³„ë„ ë¶„ì„
- Map-Reduce íŒ¨í„´ LLM ì²˜ë¦¬
"""

import asyncio
import json
import logging
from collections.abc import AsyncGenerator

import hdbscan
import numpy as np
from kiwipiepy import Kiwi
from langchain_core.prompts import ChatPromptTemplate
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP

from app.core.analytics_prompts import (
    CLUSTER_ANALYSIS_SYSTEM_PROMPT,
    META_SUMMARY_PROMPT,
    OUTLIER_ANALYSIS_PROMPT,
    SENTIMENT_ANALYSIS_PROMPT,
    SURVEY_SUMMARY_PROMPT,
)
from app.core.exceptions import AIGenerationException
from app.core.retry_policy import bedrock_retry
from app.schemas.analytics import (
    ClusterInfo,
    EmotionType,
    GEQScores,
    OutlierInfo,
    QuestionAnalysisOutput,
    QuestionAnalysisRequest,
    SentimentDistribution,
    SentimentStats,
)
from app.services.bedrock_service import BedrockService
from app.services.embedding_service import EmbeddingService

logger = logging.getLogger(__name__)


class AnalyticsService:
    """BERTopic ê¸°ë°˜ í† í”½ ëª¨ë¸ë§ + LLM ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤"""

    MIN_CLUSTER_SIZE = 3  # HDBSCAN ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
    MMR_LAMBDA = 0.7  # MMR ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„° (0=ë‹¤ì–‘ì„±, 1=ìœ ì‚¬ë„)
    MAX_REPRESENTATIVE_DOCS = 5  # ëŒ€í‘œ ë¬¸ì„œ ìµœëŒ€ ê°œìˆ˜
    MAX_KEYWORDS = 5  # c-TF-IDF í‚¤ì›Œë“œ ìµœëŒ€ ê°œìˆ˜

    # === Quality/Validity ê°€ì¤‘ì¹˜ ===
    QUALITY_WEIGHTS = {
        "FULL": 1.0,  # ì™„ì „í•œ ì‘ë‹µ - ìµœê³  ê°€ì¤‘ì¹˜
        "GROUNDED": 0.8,  # ìƒí™©ë§Œ ì„¤ëª…
        "FLOATING": 0.6,  # ê°ì •ë§Œ í‘œí˜„
        "EMPTY": 0.3,  # ë‹¨ë‹µí˜• - ë‚®ì€ ê°€ì¤‘ì¹˜
        None: 0.5,  # ë©”íƒ€ë°ì´í„° ì—†ìŒ (ê¸°ì¡´ ë°ì´í„°)
    }

    VALIDITY_WEIGHTS = {
        "VALID": 1.0,
        "AMBIGUOUS": 0.5,
        "CONTRADICTORY": 0.5,
        "OFF_TOPIC": 0.0,  # í•„í„°ë§ ëŒ€ìƒ
        "REFUSAL": 0.0,  # í•„í„°ë§ ëŒ€ìƒ
        "UNINTELLIGIBLE": 0.0,  # í•„í„°ë§ ëŒ€ìƒ
        None: 1.0,  # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´
    }

    def __init__(
        self,
        embedding_service: EmbeddingService,
        bedrock_service: BedrockService,
    ):
        self.embedding_service = embedding_service
        self.bedrock_service = bedrock_service
        self.kiwi = Kiwi()  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
        logger.info("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    # =========================================================================
    # Step 1: Data Loading
    # =========================================================================

    def _query_answers_from_chromadb(
        self, fixed_question_id: int, survey_uuid: str, filter_invalid: bool = True
    ) -> dict:
        """ChromaDBì—ì„œ íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë“¤ + ì„ë² ë”© ì¡°íšŒ"""
        try:
            # ChromaDB ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•´ ë‹¨ì¼ ì¡°ê±´ìœ¼ë¡œ ì¡°íšŒ í›„ Pythonì—ì„œ í•„í„°ë§
            # $and ì—°ì‚°ìê°€ ì¼ë¶€ ë²„ì „ì—ì„œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ
            # TODO: ChromaDB ë²„ì „ ì—…ê·¸ë ˆì´ë“œ í›„ $and ì—°ì‚°ì ì‚¬ìš©
            results = self.embedding_service.collection.get(
                where={"fixed_question_id": fixed_question_id},
                include=["documents", "metadatas", "embeddings"],
            )

            if not results["ids"]:
                logger.warning(
                    f"âš ï¸ ë‹µë³€ ì—†ìŒ: question_id={fixed_question_id}, survey_uuid={survey_uuid}"
                )
                return {"ids": [], "documents": [], "metadatas": [], "embeddings": []}

            # survey_uuid + Validity í•„í„°ë§ (Pythonì—ì„œ ì²˜ë¦¬)
            filtered_indices = []
            for i, meta in enumerate(results["metadatas"]):
                if meta.get("survey_uuid") != survey_uuid:
                    continue

                # === ì‹ ê·œ: Validity í•„í„°ë§ ===
                if filter_invalid:
                    validity = meta.get("validity")
                    if validity in ["OFF_TOPIC", "REFUSAL", "UNINTELLIGIBLE"]:
                        logger.debug(
                            f"ğŸš« Filtered out document {i} due to validity={validity}"
                        )
                        continue

                filtered_indices.append(i)

            if not filtered_indices:
                logger.warning(
                    f"âš ï¸ survey_uuid í•„í„° í›„ ë‹µë³€ ì—†ìŒ: question_id={fixed_question_id}, survey_uuid={survey_uuid}"
                )
                return {"ids": [], "documents": [], "metadatas": [], "embeddings": []}

            filtered_results = {
                "ids": [results["ids"][i] for i in filtered_indices],
                "documents": [results["documents"][i] for i in filtered_indices],
                "metadatas": [results["metadatas"][i] for i in filtered_indices],
                "embeddings": [results["embeddings"][i] for i in filtered_indices],
            }

            logger.info(f"âœ… ChromaDB ì¡°íšŒ ì™„ë£Œ: {len(filtered_results['ids'])}ê°œ ë‹µë³€")
            return filtered_results

        except Exception as error:
            logger.error(f"âŒ ChromaDB ì¡°íšŒ ì‹¤íŒ¨: {error}")
            raise AIGenerationException(f"ChromaDB ì¡°íšŒ ì‹¤íŒ¨: {error}") from error

    # =========================================================================
    # Step 2: UMAP Dimensionality Reduction
    # =========================================================================

    def _reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:
        """UMAPìœ¼ë¡œ ê³ ì°¨ì› ì„ë² ë”©ì„ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ"""
        n_samples = len(embeddings)
        if n_samples < 5:
            # ìƒ˜í”Œì´ ë„ˆë¬´ ì ìœ¼ë©´ ì°¨ì› ì¶•ì†Œ ìƒëµ
            return embeddings

        n_neighbors = min(15, n_samples - 1)
        n_components = min(5, n_samples - 1)

        umap_model = UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            min_dist=0.0,
            metric="cosine",
        )
        reduced = umap_model.fit_transform(embeddings)
        logger.info(f"âœ… UMAP ì°¨ì› ì¶•ì†Œ: {embeddings.shape[1]}d â†’ {reduced.shape[1]}d")
        return reduced

    # =========================================================================
    # Step 3: HDBSCAN Clustering
    # =========================================================================

    def _cluster_with_hdbscan(
        self, embeddings: np.ndarray
    ) -> tuple[dict[int, list[int]], list[int]]:
        """HDBSCANìœ¼ë¡œ ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ë…¸ì´ì¦ˆ ìë™ ë¶„ë¦¬)"""
        n_samples = len(embeddings)
        min_cluster_size = max(2, min(self.MIN_CLUSTER_SIZE, n_samples // 2))

        if n_samples < 20:
            min_samples = 1
        elif n_samples < 40:
            min_samples = 2
        else:
            min_samples = 3

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_method="eom",
        )
        labels = clusterer.fit_predict(embeddings)

        # í´ëŸ¬ìŠ¤í„°ë³„ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
        clusters: dict[int, list[int]] = {}
        outlier_indices: list[int] = []

        for idx, label in enumerate(labels):
            if label == -1:
                outlier_indices.append(idx)
            else:
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(idx)

        n_clusters = len(clusters)
        n_outliers = len(outlier_indices)
        logger.info(f"âœ… HDBSCAN ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, {n_outliers}ê°œ ì´ìƒì¹˜")

        return clusters, outlier_indices

    # =========================================================================
    # Step 4: c-TF-IDF Keyword Extraction
    # =========================================================================

    def _extract_keywords_ctfidf(
        self,
        documents: list[str],
        metadatas: list[dict],
        cluster_indices: dict[int, list[int]],
    ) -> dict[int, list[str]]:
        """c-TF-IDFë¡œ ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ (Kiwi í˜•íƒœì†Œ ë¶„ì„ ì ìš©)"""
        if not documents or not cluster_indices:
            return {}

        try:
            # ë‹µë³€(A:) ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
            def extract_answers_only(doc: str) -> str:
                """ë¬¸ì„œì—ì„œ 'A:' ë’¤ì˜ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
                answers = []
                for line in doc.split("\n"):
                    if line.startswith("A:"):
                        answers.append(line[2:].strip())
                return " ".join(answers) if answers else doc

            # Kiwi í† í°í™” í•¨ìˆ˜ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ)
            def tokenize_korean(text: str) -> str:
                """Kiwië¡œ í•œêµ­ì–´ í† í°í™” - ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ"""
                tokens = self.kiwi.tokenize(text)
                # ëª…ì‚¬(NNG, NNP), ë™ì‚¬(VV), í˜•ìš©ì‚¬(VA)ë§Œ ì¶”ì¶œ
                # 1ê¸€ì ë‹¨ì–´ëŠ” ì œì™¸ (ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“±)
                keywords = [
                    token.form
                    for token in tokens
                    if token.tag in ("NNG", "NNP", "VV", "VA") and len(token.form) > 1
                ]
                return " ".join(keywords)

            # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹µë³€ë§Œ í•©ì³ì„œ 'ë©”íƒ€ ë¬¸ì„œ' ìƒì„± (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
            cluster_docs = []
            cluster_labels = []
            for label, indices in cluster_indices.items():
                weighted_answers = []

                for i in indices:
                    answer = extract_answers_only(documents[i])

                    # === ì‹ ê·œ: í’ˆì§ˆ ê¸°ë°˜ ì¤‘ë³µ ===
                    quality = metadatas[i].get("quality")
                    weight = self.QUALITY_WEIGHTS.get(quality, 0.5)

                    # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì¤‘ë³µ (FULL=3íšŒ, GROUNDED=2íšŒ, ê¸°íƒ€=1íšŒ)
                    repeat_count = max(1, int(weight * 3))
                    weighted_answers.extend([answer] * repeat_count)

                combined = " ".join(weighted_answers)
                # Kiwië¡œ í† í°í™”
                tokenized = tokenize_korean(combined)
                cluster_docs.append(tokenized)
                cluster_labels.append(label)

            if not cluster_docs:
                return {}

            # CountVectorizerë¡œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            vectorizer = CountVectorizer(
                max_features=1000,
                ngram_range=(1, 1),  # Kiwiê°€ ì´ë¯¸ í† í°í™”í–ˆìœ¼ë¯€ë¡œ unigramë§Œ
            )
            tf_matrix = vectorizer.fit_transform(cluster_docs)
            feature_names = vectorizer.get_feature_names_out()

            # c-TF-IDF ê³„ì‚°: TF * log(1 + A/tf_global)
            tf_array = tf_matrix.toarray()
            global_tf = tf_array.sum(axis=0) + 1  # 0 ë‚˜ëˆ—ì…ˆ ë°©ì§€
            avg_words = tf_array.sum() / len(cluster_docs)

            ctfidf_matrix = tf_array * np.log(1 + avg_words / global_tf)

            # ê° í´ëŸ¬ìŠ¤í„°ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_by_cluster = {}
            for i, label in enumerate(cluster_labels):
                scores = ctfidf_matrix[i]
                top_indices = scores.argsort()[-self.MAX_KEYWORDS :][::-1]
                keywords = [
                    feature_names[idx] for idx in top_indices if scores[idx] > 0
                ]
                keywords_by_cluster[label] = keywords

            return keywords_by_cluster

        except Exception as error:
            logger.warning(f"âš ï¸ c-TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {error}")
            return {}

    # =========================================================================
    # Step 5: MMR Representative Document Selection
    # =========================================================================

    def _select_representatives_mmr(
        self,
        embeddings: np.ndarray,
        indices: list[int],
        metadatas: list[dict] = None,
        n_docs: int = 5,
    ) -> list[int]:
        """MMRë¡œ ëŒ€í‘œ ë¬¸ì„œ ì„ ì • (ìœ ì‚¬ë„ + ë‹¤ì–‘ì„± ê· í˜•)"""
        if len(indices) <= n_docs:
            return indices

        cluster_embeddings = embeddings[indices]
        centroid = cluster_embeddings.mean(axis=0)

        # ì •ê·œí™”
        norms = np.linalg.norm(cluster_embeddings, axis=1, keepdims=True)
        norms = np.where(norms == 0, 1, norms)
        normalized = cluster_embeddings / norms
        centroid_norm = centroid / (np.linalg.norm(centroid) + 1e-10)

        # Centroidì™€ì˜ ìœ ì‚¬ë„
        relevance = normalized @ centroid_norm

        selected = []
        remaining = list(range(len(indices)))

        for _ in range(min(n_docs, len(indices))):
            if not remaining:
                break

            if not selected:
                # ì²« ë²ˆì§¸ëŠ” ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ
                best_idx = remaining[np.argmax(relevance[remaining])]
            else:
                # MMR ì ìˆ˜ ê³„ì‚°
                mmr_scores = []
                for idx in remaining:
                    rel = relevance[idx]
                    # ì´ë¯¸ ì„ íƒëœ ë¬¸ì„œë“¤ê³¼ì˜ ìµœëŒ€ ìœ ì‚¬ë„
                    max_sim = max(normalized[idx] @ normalized[s] for s in selected)

                    # === ì‹ ê·œ: í’ˆì§ˆ ë³´ë„ˆìŠ¤ ===
                    quality_bonus = 0.0
                    if metadatas is not None:
                        quality = metadatas[indices[idx]].get("quality")
                        quality_bonus = (
                            self.QUALITY_WEIGHTS.get(quality, 0.5) * 0.2
                        )  # ìµœëŒ€ +0.2

                    mmr = (
                        self.MMR_LAMBDA * rel
                        - (1 - self.MMR_LAMBDA) * max_sim
                        + quality_bonus
                    )
                    mmr_scores.append(mmr)
                best_idx = remaining[np.argmax(mmr_scores)]

            selected.append(best_idx)
            remaining.remove(best_idx)

        # ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        return [indices[i] for i in selected]

    # =========================================================================
    # Step 6: LLM Sentiment Analysis
    # =========================================================================

    @bedrock_retry
    async def _analyze_sentiment_with_llm(self, documents: list[str]) -> dict:
        """LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ê°ì • ë¶„ì„"""
        try:
            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", CLUSTER_ANALYSIS_SYSTEM_PROMPT),
                    ("user", SENTIMENT_ANALYSIS_PROMPT),
                ]
            )
            chain = prompt | self.bedrock_service.chat_model
            docs_text = "\n".join([f"- {doc}" for doc in documents])

            response = await chain.ainvoke({"answers": docs_text})

            return self._parse_llm_json(response.content)

        except Exception as error:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {error}")
            return {
                "summary": "ë¶„ì„ ì‹¤íŒ¨",
                "emotion_detail": str(error),
                "satisfaction": 50,
                "geq_scores": {
                    "competence": 0,
                    "immersion": 0,
                    "flow": 0,
                    "tension": 0,
                    "challenge": 0,
                    "positive_affect": 0,
                    "negative_affect": 0,
                },
            }

    # =========================================================================
    # Step 7: Outlier Analysis
    # =========================================================================

    @bedrock_retry
    async def _analyze_outliers_with_llm(self, documents: list[str]) -> str:
        """LLMìœ¼ë¡œ ì´ìƒì¹˜ ë‹µë³€ ë¶„ì„"""
        if not documents:
            return "ì´ìƒì¹˜ ì—†ìŒ"

        try:
            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", CLUSTER_ANALYSIS_SYSTEM_PROMPT),
                    ("user", OUTLIER_ANALYSIS_PROMPT),
                ]
            )
            chain = prompt | self.bedrock_service.chat_model
            docs_text = "\n".join([f"- {doc}" for doc in documents[:10]])

            response = await chain.ainvoke({"answers": docs_text})

            result = self._parse_llm_json(response.content)
            return result.get("summary", "ë¶„ì„ ë¶ˆê°€")

        except Exception as error:
            logger.error(f"âŒ ì´ìƒì¹˜ ë¶„ì„ ì‹¤íŒ¨: {error}")
            return "ë¶„ì„ ì‹¤íŒ¨"

    # =========================================================================
    # Step 8: Map-Reduce Meta Summary
    # =========================================================================

    @bedrock_retry
    async def _generate_meta_summary(self, cluster_summaries: list[str]) -> str:
        """Map-Reduce: í´ëŸ¬ìŠ¤í„° ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ë©”íƒ€ ìš”ì•½ ìƒì„±"""
        if not cluster_summaries:
            return ""

        try:
            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", CLUSTER_ANALYSIS_SYSTEM_PROMPT),
                    ("user", META_SUMMARY_PROMPT),
                ]
            )
            chain = prompt | self.bedrock_service.chat_model
            summaries_text = "\n".join([f"- {s}" for s in cluster_summaries])

            response = await chain.ainvoke({"cluster_summaries": summaries_text})

            result = self._parse_llm_json(response.content)
            return result.get("meta_summary", "")

        except Exception as error:
            logger.error(f"âŒ ë©”íƒ€ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {error}")
            return ""

    # =========================================================================
    # Step 9: Survey Summary
    # =========================================================================

    @bedrock_retry
    async def generate_survey_summary(self, question_summaries: list[str]) -> str:
        """ê° ì§ˆë¬¸ë³„ ìš”ì•½ì„ ì¢…í•©í•˜ì—¬ ì„¤ë¬¸ ì „ì²´ í‰ê°€ ìƒì„±"""
        if not question_summaries:
            return ""

        try:
            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", CLUSTER_ANALYSIS_SYSTEM_PROMPT),
                    ("user", SURVEY_SUMMARY_PROMPT),
                ]
            )
            chain = prompt | self.bedrock_service.chat_model
            summaries_text = "\n".join(
                [f"- Q{i + 1}: {s}" for i, s in enumerate(question_summaries)]
            )

            response = await chain.ainvoke({"question_summaries": summaries_text})

            result = self._parse_llm_json(response.content)
            return result.get("survey_summary", "")

        except Exception as error:
            logger.error(f"âŒ ì„¤ë¬¸ ì¢…í•© í‰ê°€ ìƒì„± ì‹¤íŒ¨: {error}")
            return ""

    def _parse_llm_json(self, content) -> dict:
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        if isinstance(content, list):
            content = content[0].get("text", "") if content else ""

        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]

        try:
            return json.loads(content.strip())
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"íŒŒì‹± ì‹¤íŒ¨í•œ ë‚´ìš©: {content[:500]}")  # ì²˜ìŒ 500ìë§Œ ë¡œê¹…
            return {}

    def _map_emotion_type(self, emotion_str: str) -> EmotionType:
        """ë¬¸ìì—´ì„ EmotionTypeìœ¼ë¡œ ë³€í™˜"""
        try:
            return EmotionType(emotion_str)
        except ValueError:
            # ê¸°ë³¸ê°’ ë§¤í•‘
            if emotion_str == "ì¤‘ë¦½":
                return EmotionType.NEUTRAL
            return EmotionType.NEUTRAL

    def _calculate_sentiment_stats(self, clusters: list[ClusterInfo]) -> SentimentStats:
        """í´ëŸ¬ìŠ¤í„° ì •ë³´ë¡œë¶€í„° ì „ì²´ ê°ì • í†µê³„ ê³„ì‚°"""
        if not clusters:
            return SentimentStats(
                score=50,
                label="ì¤‘ë¦½",
                distribution=SentimentDistribution(positive=0, neutral=100, negative=0),
            )

        # ì ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜ (60ì  ì´ìƒ ê¸ì •, 40ì  ì´í•˜ ë¶€ì •)

        positive_count = 0
        negative_count = 0
        neutral_count = 0
        total_weighted_score = 0

        for cluster in clusters:
            count = cluster.count
            score = cluster.satisfaction

            # ê°€ì¤‘ í‰ê· ìš© ì ìˆ˜ ì ë¦½
            total_weighted_score += count * score

            # ë¶„í¬ ì§‘ê³„ (ì ìˆ˜ ê¸°ë°˜ ë‹¨ìˆœ ë¶„ë¥˜)
            if score >= 60:
                positive_count += count
            elif score <= 40:
                negative_count += count
            else:
                neutral_count += count

        total = positive_count + negative_count + neutral_count or 1

        # ë¶„í¬ ê³„ì‚° (í•©ê³„ 100% ë³´ì¥)
        positive_pct = round((positive_count / total) * 100)
        negative_pct = round((negative_count / total) * 100)
        neutral_pct = 100 - positive_pct - negative_pct  # ë‚˜ë¨¸ì§€ë¡œ 100% ë³´ì¥

        distribution = SentimentDistribution(
            positive=positive_pct,
            neutral=neutral_pct,
            negative=negative_pct,
        )

        # 4. ìµœì¢… ë¼ë²¨ ë° ì ìˆ˜ íŒì •
        # ì ìˆ˜ëŠ” ì´ì œ "ì¸ì›ìˆ˜ ê°€ì¤‘ í‰ê·  ë§Œì¡±ë„" (0~100)
        score = round(total_weighted_score / total)
        if score >= 60:
            label = "ê¸ì •"
        elif score <= 40:
            label = "ë¶€ì •"
        else:
            label = "ì¤‘ë¦½"

        return SentimentStats(score=score, label=label, distribution=distribution)

    # =========================================================================
    # Main Pipeline (SSE Streaming)
    # =========================================================================

    async def stream_analysis(
        self, question_id: int, request: QuestionAnalysisRequest
    ) -> AsyncGenerator[str, None]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜"""
        try:
            logger.info(
                f"ğŸ” ë¶„ì„ ì‹œì‘: Question {question_id}, Survey {request.survey_uuid}, FixedQuestion {request.fixed_question_id}"
            )

            # Step 1: Progress - Loading
            yield f"event: progress\ndata: {json.dumps({'step': 'loading', 'progress': 10})}\n\n"

            # Step 2: ChromaDB ì¡°íšŒ
            results = self._query_answers_from_chromadb(
                request.fixed_question_id, request.survey_uuid
            )
            total_count = len(results["ids"])

            if total_count == 0:
                yield f"event: error\ndata: {json.dumps({'message': 'ë¶„ì„í•  ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.'})}\n\n"
                return

            yield f"event: progress\ndata: {json.dumps({'step': 'loaded', 'progress': 20, 'answer_count': total_count})}\n\n"

            ids = results["ids"]
            documents = results["documents"]
            metadatas = results["metadatas"]  # â† ì¶”ê°€
            embeddings = np.array(results["embeddings"])

            # Step 3: UMAP ì°¨ì› ì¶•ì†Œ
            yield f"event: progress\ndata: {json.dumps({'step': 'reducing', 'progress': 30})}\n\n"
            reduced_embeddings = self._reduce_dimensions(embeddings)

            # Step 4: HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
            yield f"event: progress\ndata: {json.dumps({'step': 'clustering', 'progress': 40})}\n\n"
            cluster_indices, outlier_indices = self._cluster_with_hdbscan(
                reduced_embeddings
            )

            # Step 5: c-TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (ê°€ì¤‘ì¹˜ ì ìš©)
            yield f"event: progress\ndata: {json.dumps({'step': 'extracting_keywords', 'progress': 50})}\n\n"
            keywords_by_cluster = self._extract_keywords_ctfidf(
                documents,
                metadatas,
                cluster_indices,  # â† metadatas ì¶”ê°€
            )

            # Step 6: í´ëŸ¬ìŠ¤í„°ë³„ LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
            yield f"event: progress\ndata: {json.dumps({'step': 'analyzing', 'progress': 60})}\n\n"

            # í´ëŸ¬ìŠ¤í„°ë³„ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¤€ë¹„
            cluster_metadata = []
            llm_tasks = []

            for cluster_label, indices in cluster_indices.items():
                # MMRë¡œ ëŒ€í‘œ ë¬¸ì„œ ì„ ì • (í’ˆì§ˆ ë³´ë„ˆìŠ¤ ì ìš©)
                rep_indices = self._select_representatives_mmr(
                    embeddings,
                    indices,
                    metadatas,
                    self.MAX_REPRESENTATIVE_DOCS,  # â† metadatas ì¶”ê°€
                )
                rep_docs = [documents[i] for i in rep_indices]

                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ í›„ ê²°ê³¼ ì¡°í•©ìš©)
                cluster_metadata.append(
                    {
                        "cluster_label": cluster_label,
                        "indices": indices,
                        "rep_indices": rep_indices,
                        "count": len(indices),
                        "percentage": round((len(indices) / total_count) * 100),
                        "keywords": keywords_by_cluster.get(cluster_label, []),
                    }
                )

                # LLM ê°ì • ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
                llm_tasks.append(self._analyze_sentiment_with_llm(rep_docs))

            # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            logger.info(f"â³ ë³‘ë ¬ LLM ë¶„ì„ ì‹œì‘: {len(llm_tasks)}ê°œ í´ëŸ¬ìŠ¤í„°")
            sentiments = await asyncio.gather(*llm_tasks, return_exceptions=True)
            logger.info(f"âœ… ë³‘ë ¬ LLM ë¶„ì„ ì™„ë£Œ: {len(sentiments)}ê°œ ê²°ê³¼")

            # ì˜ˆì™¸ ì²´í¬ ë° ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            for i, result in enumerate(sentiments):
                if isinstance(result, Exception):
                    logger.error(
                        f"âŒ í´ëŸ¬ìŠ¤í„° {i} ë¶„ì„ ì˜ˆì™¸: {type(result).__name__}: {result}"
                    )
                    sentiments[i] = {
                        "summary": "ë¶„ì„ ì‹¤íŒ¨",
                        "emotion_detail": str(result),
                        "satisfaction": 50,
                        "geq_scores": {},
                    }

            cluster_infos = []
            cluster_summaries = []  # Map-Reduceìš©

            for metadata, sentiment in zip(cluster_metadata, sentiments, strict=True):
                summary = sentiment.get(
                    "summary", f"í´ëŸ¬ìŠ¤í„° {metadata['cluster_label'] + 1}"
                )
                cluster_summaries.append(summary)

                # GEQ ì ìˆ˜ íŒŒì‹±
                raw_scores = sentiment.get("geq_scores", {})
                geq_scores = GEQScores(
                    competence=min(100, max(0, raw_scores.get("competence", 0))),
                    immersion=min(100, max(0, raw_scores.get("immersion", 0))),
                    flow=min(100, max(0, raw_scores.get("flow", 0))),
                    tension=min(100, max(0, raw_scores.get("tension", 0))),
                    challenge=min(100, max(0, raw_scores.get("challenge", 0))),
                    positive_affect=min(
                        100, max(0, raw_scores.get("positive_affect", 0))
                    ),
                    negative_affect=min(
                        100, max(0, raw_scores.get("negative_affect", 0))
                    ),
                )

                # ì£¼ìš” ê°ì • (ê°€ì¥ ë†’ì€ ì ìˆ˜)
                dominant_emotion = geq_scores.get_dominant_emotion()

                cluster_infos.append(
                    ClusterInfo(
                        summary=summary,
                        percentage=metadata["percentage"],
                        count=metadata["count"],
                        emotion_type=self._map_emotion_type(dominant_emotion),
                        geq_scores=geq_scores,
                        emotion_detail=sentiment.get("emotion_detail", ""),
                        answer_ids=[ids[i] for i in metadata["indices"]],
                        satisfaction=sentiment.get("satisfaction", 50),
                        keywords=metadata["keywords"],
                        representative_answers=[
                            documents[i] for i in metadata["rep_indices"]
                        ],
                    )
                )

            # ë¹„ì¤‘ ìˆœ ì •ë ¬
            cluster_infos.sort(key=lambda c: c.count, reverse=True)

            # Step 7 & 8: ì´ìƒì¹˜ ë¶„ì„ + ë©”íƒ€ ìš”ì•½ (ë³‘ë ¬ ì²˜ë¦¬)
            yield f"event: progress\ndata: {json.dumps({'step': 'finalizing', 'progress': 85})}\n\n"

            # ë³‘ë ¬ íƒœìŠ¤í¬ ì¤€ë¹„
            outlier_task = None
            outlier_docs = []
            rep_outlier_indices = []

            if outlier_indices:
                # ì´ìƒì¹˜ ì¤‘ì—ì„œë„ ë‹¤ì–‘í•œ 'ëŒ€í‘œ ì´ìƒì¹˜' ì„ ì • (MMR ì ìš©)
                # ì´ìƒì¹˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë¹„ìš© ë¬¸ì œ ë°œìƒí•˜ë¯€ë¡œ ìµœëŒ€ 10ê°œë§Œ ì„ ì •
                rep_outlier_indices = self._select_representatives_mmr(
                    embeddings, outlier_indices, n_docs=10
                )
                outlier_docs = [documents[i] for i in rep_outlier_indices]
                outlier_task = self._analyze_outliers_with_llm(outlier_docs)

            meta_task = self._generate_meta_summary(cluster_summaries)

            # ì´ìƒì¹˜ ë¶„ì„ê³¼ ë©”íƒ€ ìš”ì•½ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            if outlier_task:
                outlier_summary, meta_summary = await asyncio.gather(
                    outlier_task, meta_task
                )
                outlier_info = OutlierInfo(
                    count=len(outlier_indices),
                    summary=outlier_summary,
                    answer_ids=[ids[i] for i in outlier_indices],
                    sample_answers=outlier_docs,
                )
            else:
                meta_summary = await meta_task
                outlier_info = None

            logger.info("âœ… ì´ìƒì¹˜ ë¶„ì„ + ë©”íƒ€ ìš”ì•½ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

            # Step 9: ì „ì²´ í†µê³„ ê³„ì‚°
            sentiment_stats = self._calculate_sentiment_stats(cluster_infos)

            output = QuestionAnalysisOutput(
                question_id=question_id,
                total_answers=total_count,
                clusters=cluster_infos,
                sentiment=sentiment_stats,
                outliers=outlier_info,
                meta_summary=meta_summary if meta_summary else None,
            )

            logger.info(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì§ë ¬í™” ì‹œì‘: Question {question_id}")
            try:
                output_json = output.model_dump_json()
                logger.info(
                    f"ğŸ“¤ SSE done ì´ë²¤íŠ¸ ì „ì†¡: Question {question_id}, í¬ê¸°={len(output_json)}ë°”ì´íŠ¸"
                )
                yield f"event: done\ndata: {output_json}\n\n"
                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: Question {question_id}")
            except Exception as serialize_error:
                logger.error(
                    f"âŒ ì§ë ¬í™” ì‹¤íŒ¨: {type(serialize_error).__name__}: {serialize_error}",
                    exc_info=True,
                )
                yield 'event: error\ndata: {"message": "ê²°ê³¼ ì§ë ¬í™” ì‹¤íŒ¨"}\n\n'

        except AIGenerationException as error:
            logger.error(
                f"âŒ AI ìƒì„± ì˜¤ë¥˜ (Question {question_id}): {error}", exc_info=True
            )
            yield f"event: error\ndata: {json.dumps({'message': str(error)})}\n\n"
        except Exception as error:
            logger.error(
                f"âŒ ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ (Question {question_id}): {error}",
                exc_info=True,
            )
            yield f"event: error\ndata: {json.dumps({'message': 'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'})}\n\n"
