"""
ì„¤ë¬¸/ì¸í„°ë·° ìƒí˜¸ì‘ìš© ì„œë¹„ìŠ¤ (Phase 3-4: ê³ ì •ì§ˆë¬¸ + ê¼¬ë¦¬ì§ˆë¬¸)
ì‚¬ìš©ì ë‹µë³€ ë¶„ì„ ë° ë‹¤ìŒ í–‰ë™ ê²°ì •, í”¼ë¡œë„/ì»¤ë²„ë¦¬ì§€ ì²´í¬
"""

import json
import logging
from collections.abc import AsyncGenerator
from typing import TYPE_CHECKING

from app.agents.conversation_workflow import build_survey_graph
from app.core.exceptions import AIGenerationException
from app.schemas.survey import (
    EndReason,
    InterviewPhase,
    SurveyAction,
    SurveyInteractionRequest,
    SurveyInteractionResponse,
)

if TYPE_CHECKING:
    from app.services.bedrock_service import BedrockService

logger = logging.getLogger(__name__)

# í”¼ë¡œë„ íŒë‹¨ ê¸°ì¤€
MIN_ANSWER_LENGTH_FOR_FATIGUE = 10  # ë‹µë³€ì´ ì´ ê¸¸ì´ ë¯¸ë§Œì´ë©´ í”¼ë¡œ ì‹ í˜¸
CONSECUTIVE_SHORT_ANSWERS_THRESHOLD = 3  # ì—°ì† ì§§ì€ ë‹µë³€ íšŸìˆ˜


class InteractionService:
    """ì„¤ë¬¸/ì¸í„°ë·° ìƒí˜¸ì‘ìš© ì„œë¹„ìŠ¤ (LangGraph Wrapper)"""

    def __init__(self, bedrock_service: "BedrockService"):
        self.bedrock_service = bedrock_service
        self.graph = build_survey_graph(bedrock_service)

    # =========================================================================
    # ë™ê¸° ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜)
    # =========================================================================

    def process_interaction(
        self, request: SurveyInteractionRequest
    ) -> SurveyInteractionResponse:
        """ì‚¬ìš©ì ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤ (ë™ê¸°)."""
        input_state = {
            "session_id": request.session_id,
            "user_answer": request.user_answer,
            "current_question": request.current_question,
            "game_info": request.game_info,
            "conversation_history": request.conversation_history,
        }

        config = {"configurable": {"thread_id": request.session_id}}

        try:
            final_state = self.graph.invoke(input_state, config=config)

            action = final_state.get("action")
            message = final_state.get("message")
            analysis = final_state.get("analysis")

            if not action:
                logger.warning(
                    f"âš ï¸ Action not found. Defaulting to PASS_TO_NEXT. State: {final_state}"
                )
                action = SurveyAction.PASS_TO_NEXT.value

            return SurveyInteractionResponse(
                action=action, message=message, analysis=analysis
            )

        except Exception as e:
            logger.error(f"âŒ Interaction Graph Error: {e}")
            raise AIGenerationException(f"ì„¤ë¬¸ ìƒí˜¸ì‘ìš© ì²˜ë¦¬ ì‹¤íŒ¨: {e}") from e

    # =========================================================================
    # SSE ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ (ë©”ì¸)
    # =========================================================================

    async def stream_interaction(
        self, request: SurveyInteractionRequest
    ) -> AsyncGenerator[str, None]:
        """
        SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ë¶„ì„ ë° ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±.
        
        ì´ë²¤íŠ¸ ìˆœì„œ:
        1. start: ì²˜ë¦¬ ì‹œì‘
        2. analyze_answer: ë‹µë³€ ë¶„ì„ ê²°ê³¼ (action, analysis, should_end)
        3. continue (ë°˜ë³µ): ê¼¬ë¦¬ì§ˆë¬¸ í† í° ìŠ¤íŠ¸ë¦¬ë° (TAIL_QUESTIONì¸ ê²½ìš°)
        4. generate_tail_complete: ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì™„ë£Œ (TAIL_QUESTIONì¸ ê²½ìš°)
        5. done: ì²˜ë¦¬ ì™„ë£Œ
        """
        try:
            yield self._sse_event("start", {"status": "processing", "phase": "main"})

            # í”¼ë¡œë„ ì²´í¬ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            fatigue_check = self._check_fatigue(request)

            # Step 1: ë‹µë³€ ë¶„ì„
            analyze_result = await self.bedrock_service.analyze_answer_async(
                current_question=request.current_question,
                user_answer=request.user_answer,
                tail_question_count=request.probe_count,
                game_info=request.game_info,
                conversation_history=request.conversation_history,
            )

            # í”¼ë¡œë„ê°€ ë†’ìœ¼ë©´ AI íŒë‹¨ê³¼ ë³„ê°œë¡œ ì¢…ë£Œ ê¶Œì¥
            should_end = fatigue_check["fatigued"]
            end_reason = EndReason.FATIGUE.value if should_end else None

            yield self._sse_event("analyze_answer", {
                "action": analyze_result["action"],
                "analysis": analyze_result["analysis"],
                "should_end": should_end,
                "end_reason": end_reason,
            })

            # Step 2: ê¼¬ë¦¬ ì§ˆë¬¸ í•„ìš” ì‹œ í† í° ìŠ¤íŠ¸ë¦¬ë°
            action = analyze_result["action"]
            full_message = ""

            if action == SurveyAction.TAIL_QUESTION.value and not should_end:
                async for token in self.bedrock_service.stream_tail_question(
                    current_question=request.current_question,
                    user_answer=request.user_answer,
                    game_info=request.game_info,
                    conversation_history=request.conversation_history,
                ):
                    full_message += token
                    # token â†’ continue ì´ë²¤íŠ¸ë¡œ ë³€ê²½
                    yield self._sse_event("continue", {"content": token})

                # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì™„ë£Œ
                yield self._sse_event("generate_tail_complete", {
                    "message": full_message,
                    "tail_question_count": request.probe_count + 1,
                })

            # ì™„ë£Œ ì´ë²¤íŠ¸ (phase, should_end í¬í•¨)
            yield self._sse_event("done", {
                "status": "completed",
                "action": action,
                "phase": InterviewPhase.MAIN.value,
                "question_text": full_message if full_message else None,
                "should_end": should_end,
                "end_reason": end_reason,
            })

        except Exception as e:
            logger.error(f"âŒ Streaming Error: {e}")
            yield self._sse_event("error", {"message": str(e)})

    # =========================================================================
    # í”¼ë¡œë„ ì²´í¬ (AI íŒë‹¨ ë³´ì¡°)
    # =========================================================================

    def _check_fatigue(self, request: SurveyInteractionRequest) -> dict:
        """
        í…ŒìŠ¤í„° í”¼ë¡œë„ë¥¼ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì²´í¬.
        
        ê¸°ì¤€:
        - ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ (10ì ë¯¸ë§Œ)
        - ì—°ì†ìœ¼ë¡œ ì§§ì€ ë‹µë³€ (ëŒ€í™” ê¸°ë¡ì—ì„œ í™•ì¸)
        """
        current_answer_short = len(request.user_answer.strip()) < MIN_ANSWER_LENGTH_FOR_FATIGUE

        # ëŒ€í™” ê¸°ë¡ì—ì„œ ì—°ì† ì§§ì€ ë‹µë³€ ì²´í¬
        consecutive_short = 0
        if request.conversation_history:
            for entry in reversed(request.conversation_history):
                answer = entry.get("answer", "")
                if len(answer.strip()) < MIN_ANSWER_LENGTH_FOR_FATIGUE:
                    consecutive_short += 1
                else:
                    break

        if current_answer_short:
            consecutive_short += 1

        fatigued = consecutive_short >= CONSECUTIVE_SHORT_ANSWERS_THRESHOLD

        if fatigued:
            logger.info(
                f"ğŸ˜“ Fatigue detected: {consecutive_short} consecutive short answers"
            )

        return {
            "fatigued": fatigued,
            "consecutive_short": consecutive_short,
        }

    # =========================================================================
    # Helper
    # =========================================================================

    def _sse_event(self, event_type: str, data: dict) -> str:
        """SSE ì´ë²¤íŠ¸ í¬ë§· ìƒì„±"""
        payload = {"event": event_type, "data": data}
        return f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
